{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: str):\n",
    "    \"\"\"\n",
    "    This function loads data from a pickle file located at the provided path.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): The path to the pickle file.\n",
    "\n",
    "    Returns:\n",
    "        par_comb (np.ndarray): The parameter combinations.\n",
    "        S11_par (np.ndarray): The best parametric data.\n",
    "        frequency (np.ndarray): The frequency data.\n",
    "        degrees (np.ndarray): The degrees data.\n",
    "        combined_gain (np.ndarray): The combined gain list.\n",
    "        std_dev (np.ndarray): The standard deviation of Phi.\n",
    "        efficiency (np.ndarray): The efficiency data.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path,'rb') as file:\n",
    "        data_dict = pickle.load(file)\n",
    "    print(f\"Dictionary keys: {data_dict.keys()}\")\n",
    "\n",
    "    par_comb = np.asarray(data_dict['Parameter combination'])\n",
    "    S11_vals = np.asarray(data_dict['S1,1'])\n",
    "    frequency = np.asarray(data_dict['Frequency'])\n",
    "    S11_parametrized = np.asarray(data_dict['Parametric S1,1'])\n",
    "    degrees = np.asarray(data_dict['degrees'])\n",
    "    combined_gain = np.asarray(data_dict['combined gain list'])\n",
    "    std_dev = np.asarray(data_dict['Standard deviation Phi'])\n",
    "    efficiency = np.asarray(data_dict['efficiency'])\n",
    "    #efficiency = np.asarray(list(data_dict['efficiency'].values()))\n",
    "    return par_comb, S11_vals, S11_parametrized, frequency, degrees, combined_gain, std_dev, efficiency\n",
    "\n",
    "def normalize_data(data, inverse: bool):\n",
    "    if inverse:\n",
    "        data_norm = data*np.std(data) + np.mean(data)\n",
    "    else:   \n",
    "        mean = np.mean(data)\n",
    "        std = np.std(data)\n",
    "        data_norm = (data-mean)/std\n",
    "    return data_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nlyho/Desktop/MachineLearning/'\n",
    "par_comb, S11_vals, S11_parameterized, frequency, degrees, combined_gain, std_dev, efficiency = load_data(f\"{path}data/simple_wire2_final_with_parametric.pkl\")\n",
    "\n",
    "# Normalize data\n",
    "# S11_vals = normalize_data(S11_vals, False)\n",
    "# S11_parameterized = normalize_data(S11_parameterized, False)\n",
    "# frequency = normalize_data(frequency, False)\n",
    "# degrees = normalize_data(degrees, False)\n",
    "# combined_gain = normalize_data(combined_gain, False)\n",
    "# std_dev = normalize_data(std_dev, False)\n",
    "# efficiency = normalize_data(efficiency, False)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(par_comb, S11_vals, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Find test curves where the S11 goes below -10 dB and the frequency is below 2 GHz\n",
    "test_indices = []\n",
    "for idx, i in enumerate(y_test):\n",
    "    if np.min(i) < -10 and frequency[np.argmin(i)] < 2000:\n",
    "        test_indices.append(idx)\n",
    "print(f\"Number of test curves that satisfy the condition: {len(test_indices)}\")\n",
    "random_indices = random.sample(test_indices, 10)\n",
    "\n",
    "# # plot random test data curves to verify that a good solution is within the test set\n",
    "# plt.figure(figsize=(50, 50))\n",
    "# for idx, i in enumerate(random_indices):\n",
    "#     plt.subplot(5, 2, idx+1)\n",
    "#     plt.plot(frequency, y_test[i][0:1001], label='test')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.ylim([-40,2])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a 4 layer neural network using relu activation functions\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(y_train.shape[1], activation='linear'))\n",
    "\n",
    "model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss=keras.losses.MeanAbsoluteError(),\n",
    "            metrics=[keras.metrics.MeanSquaredError()]\n",
    "        )\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=1000,\n",
    "                    batch_size=100,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the training and validation loss\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.plot(history.history['loss'], label='Training loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# # Plot the training and validation mae\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.plot(history.history['mae'], label='Training mae')\n",
    "# plt.plot(history.history['val_mae'], label='Validation mae')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {test_loss}\")\n",
    "print(f\"Test mae: {test_mae}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Plot the predicted S11 values\n",
    "plt.figure(figsize=(50, 50))\n",
    "for idx, i in enumerate(random_indices):\n",
    "    plt.subplot(5, 2, idx+1)\n",
    "    plt.plot(frequency, y_test[i][0:1001], label='test')\n",
    "    plt.plot(frequency, y_pred[i][0:1001], label='predicted')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.ylim([-40,2])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
